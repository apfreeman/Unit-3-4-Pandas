{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  A Whale off the Port(folio)\n",
    " ---\n",
    "\n",
    " In this assignment, you'll get to use what you've learned this week to evaluate the performance among various algorithmic, hedge, and mutual fund portfolios and compare them against the S&P TSX 60 Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the path library to import path\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, you will need to read the CSV files into DataFrames and perform any necessary data cleaning steps. After cleaning, combine all DataFrames into a single DataFrame.\n",
    "\n",
    "Files:\n",
    "\n",
    "* `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "* `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "* `sp_tsx_history.csv`: Contains historical closing prices of the S&P TSX 60 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whale Returns\n",
    "\n",
    "Read the Whale Portfolio daily returns and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading whale returns\n",
    "#Set csvpath to location of whale_returns csv file\n",
    "whale_returns_csv = Path(\"Resources/whale_returns.csv\")\n",
    "\n",
    "#Read in the CSV and create Dataframe, \n",
    "#Set index to date column and ensure dates are converted to a DateTimeIndex.\n",
    "#Parse dates using default pandas date parser\n",
    "whale_returns = pd.read_csv(whale_returns_csv, index_col='Date', infer_datetime_format=True, parse_dates = True)\n",
    "\n",
    "#Confirm dates are sorted in place in ascending manner\n",
    "whale_returns.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "#Return the fist three rows of the whale_returns data frame for inspection\n",
    "whale_returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count nulls\n",
    "whale_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls in place\n",
    "whale_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "whale_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Daily Returns\n",
    "\n",
    "Read the algorithmic daily returns and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading whale returns\n",
    "#Set csvpath to location of algo_returns csv file\n",
    "algo_returns_csv = Path(\"Resources/algo_returns.csv\")\n",
    "\n",
    "#Read in the CSV and create Dataframe, \n",
    "#Set index to date column and ensure dates are converted to a DateTimeIndex.\n",
    "#Parse dates using default pandas date parser\n",
    "algo_returns = pd.read_csv(algo_returns_csv, index_col='Date', infer_datetime_format=True, parse_dates = True)\n",
    "\n",
    "#Confirm dates are sorted in place in ascending manner\n",
    "algo_returns.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "#Return the fist three rows of the algo_returns data frame for inspection\n",
    "algo_returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count nulls\n",
    "algo_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls in place\n",
    "algo_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "algo_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P TSX 60 Returns\n",
    "\n",
    "Read the S&P TSX 60 historic closing prices and create a new daily returns DataFrame from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading S&P TSX 60 Closing Prices\n",
    "#Set csvpath to location of sp tsx csv file\n",
    "sp_tsx_returns_csv = Path(\"Resources/sp_tsx_history.csv\")\n",
    "\n",
    "#Read in the CSV and create Dataframe, \n",
    "#Set index to date column and ensure dates are converted to a DateTimeIndex.\n",
    "#Parse dates using default pandas date parser\n",
    "sp_tsx_returns = pd.read_csv(sp_tsx_returns_csv, index_col='Date', infer_datetime_format=True, parse_dates = True)\n",
    "\n",
    "#Confirm dates are sorted in place in ascending manner\n",
    "sp_tsx_returns.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "#Return the fist three rows of the sp_tsx_returns data frame for inspection\n",
    "sp_tsx_returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Data Types\n",
    "sp_tsx_returns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix Data Types\n",
    "#`Close` currently set to type object, Cast `Close` object as float\n",
    "#Before we can cast this as type float we will need to clean up the data and remove $ and , chars then cast as float\n",
    "sp_tsx_returns['Close'] = sp_tsx_returns['Close'].str.replace(',', '').str.replace('$', '').astype(float)\n",
    "\n",
    "#Check Data Types to confirm Close is now dtype float\n",
    "sp_tsx_returns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Daily Returns\n",
    "sp_tsx_daily_returns = sp_tsx_returns.pct_change()\n",
    "\n",
    "#Check pct_change Data\n",
    "sp_tsx_daily_returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Nulls\n",
    "sp_tsx_daily_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Drop nulls in place\n",
    "sp_tsx_daily_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "sp_tsx_daily_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename `Close` column to refelect the new data it contains which is the \"S&P TSX 60 Daily Returns\"\n",
    "sp_tsx_daily_returns = sp_tsx_daily_returns.rename(columns = {\"Close\":\"S&P TSX 60\"})\n",
    "\n",
    "#Check Data Frame to confirm column rename\n",
    "sp_tsx_daily_returns.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Whale, Algorithmic, and S&P TSX 60 Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Whale Returns, Algorithmic Returns, and the S&P TSX 60 Returns into a single DataFrame with columns for each portfolio's returns.\n",
    "combined_df = pd.concat([whale_returns, algo_returns, sp_tsx_daily_returns], axis=\"columns\", join=\"inner\")\n",
    "\n",
    "#Return the fist three rows of the new combined data frame for inspection\n",
    "combined_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Quantitative Analysis\n",
    "\n",
    "In this section, you will calculate and visualize performance and risk metrics for the portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Anlysis\n",
    "\n",
    "#### Calculate and Plot the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot daily returns of all portfolios with title \"Daily Returns\"\n",
    "combined_df.plot(figsize=(20,10), title = \"Daily Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate cumulative returns of all portfolios\n",
    "cumulative_returns = (1 + combined_df).cumprod()\n",
    "\n",
    "#Plot cumulative returns\n",
    "cumulative_returns.plot(figsize=(20,10), title = \"Cumulative Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns.\n",
    "\n",
    "\n",
    "##### From analysing the cumulative returns plot you can see that no other portfolio outperforms the S&P TSX 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "Determine the _risk_ of each portfolio:\n",
    "\n",
    "1. Create a box plot for each portfolio. \n",
    "2. Calculate the standard deviation for all portfolios.\n",
    "4. Determine which portfolios are riskier than the S&P TSX 60.\n",
    "5. Calculate the Annualized Standard Deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a box plot for each portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot to visually show risk\n",
    "combined_df.boxplot(figsize = (20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the daily standard deviations of all portfolios\n",
    "combined_df_std = combined_df.std()\n",
    "\n",
    "#Return all rows of the combined standard deviations data frame for inspection\n",
    "combined_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which portfolios are riskier than the S&P TSX 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate  the daily standard deviation of S&P TSX 60\n",
    "sp_tsx_returns_std = combined_df_std[\"S&P TSX 60\"]\n",
    "\n",
    "#Determine which portfolios are riskier than the S&P TSX 60\n",
    "#If the standard deviation for each portfolio is greater then the standard deviation of the S&P TSX 60, then 'riskier' will be True\n",
    "#Create this dataset and create a new dataframe called sp_tsx_returns_std_risk\n",
    "sp_tsx_returns_std_risk  = combined_df_std > sp_tsx_returns_std\n",
    "\n",
    "#Return the sp_tsx_returns_std_risk data frame for inspection and note any portfolios with value \"True\". These are riskier then the S&P TSX 60\n",
    "sp_tsx_returns_std_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine which portfolios are riskier than the S&P TSX 60\n",
    "\n",
    "\n",
    "##### From analysing the output below you can see the following portfolios are riskier than the S&P TSX 60\n",
    "###### SOROS FUND MANAGEMENT LLC    \n",
    "###### TIGER GLOBAL MANAGEMENT LLC     \n",
    "###### BERKSHIRE HATHAWAY INC          \n",
    "###### Algo 1                          \n",
    "###### Algo 2                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized standard deviation (252 trading days)\n",
    "annual_std = combined_df_std * np.sqrt(252)\n",
    "\n",
    "#Return annual standard deviation dataframe for inspection\n",
    "annual_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "\n",
    "Risk changes over time. Analyze the rolling statistics for Risk and Beta. \n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for all portfolios using a 21-day window.\n",
    "2. Calculate the correlation between each stock to determine which portfolios may mimick the S&P TSX 60.\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta for it and the S&P TSX 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` for all portfolios with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling standard deviation for all portfolios using a 21-day window\n",
    "group_rolling = combined_df.rolling(window=21).std()\n",
    "\n",
    "# Plot the rolling standard deviation\n",
    "group_rolling.plot(figsize = (20,10), title = \"21 day rolling standard deviation for all portfolios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "correlation = combined_df.corr()\n",
    "correlation\n",
    "\n",
    "# Display de correlation matrix\n",
    "sns.heatmap(correlation, vmin = -1, vmax = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Beta for Algo 1 portfolio and the S&P 60 TSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance of the Algo 1 Portfolio\n",
    "rolling_covariance_algo1 = combined_df['Algo 1'].rolling(window = 60).cov(combined_df['S&P TSX 60'])\n",
    "\n",
    "# Calculate variance of S&P TSX\n",
    "rolling_variance = combined_df['S&P TSX 60'].rolling(window = 60).var()\n",
    "\n",
    "# Computing beta\n",
    "rolling_beta_algo1 = rolling_covariance_algo1 / rolling_variance\n",
    "\n",
    "# Plot beta trend\n",
    "rolling_beta_algo1.plot(figsize = (20,10), title=\"Rolling Beta for Algo 1 Portfolio and the S&P TSX 60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics Challenge: Exponentially Weighted Average \n",
    "\n",
    "An alternative way to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half-life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `ewm` to calculate the rolling window\n",
    "ewm = combined_df.ewm(halflife = 21).mean().plot(figsize=(20,10), title=\"Exponentially Weighted Average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe Ratios\n",
    "In reality, investment managers and thier institutional investors look at the ratio of return-to-risk, and not just returns alone. After all, if you could invest in one of two portfolios, and each offered the same 10% return, yet one offered lower risk, you'd take that one, right?\n",
    "\n",
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\n",
    "sharpe_ratios = (combined_df.mean() * 252) / (combined_df.std() * np.sqrt(252))\n",
    "sharpe_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "sharpe_ratios.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine whether the algorithmic strategies outperform both the market (S&P TSX 60) and the whales portfolios.\n",
    "\n",
    "Sharpe ratio is a measure of excess portfolio return over the risk-free rate relative to its standard deviation. As such a higher Sharpe ratio can be considered superior to others.\n",
    "\n",
    "Both algorithmic strategies outperform the market (S&P TSX 60) \n",
    "\n",
    "The algorithmic 1 portfolio outperforms all other whale portfolio strategies. \n",
    "\n",
    "The algorithmic 2 portfolio outperforms all other whale portfolio strategies with only one exception. The only portfolio to outperform The algorithmic 2 portfolio is the BERKSHIRE HATHAWAY INC porfolio.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Portfolio\n",
    "\n",
    "In this section, you will build your own portfolio of stocks, calculate the returns, and compare the results to the Whale Portfolios and the S&P TSX 60. \n",
    "\n",
    "1. Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "2. Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock.\n",
    "3. Join your portfolio returns to the DataFrame that contains all of the portfolio returns.\n",
    "4. Re-run the performance and risk analysis with your portfolio to see how it compares to the others.\n",
    "5. Include correlation analysis to determine which stocks (if any) are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock 1 Walmart - Import and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from 1st stock - Walmart\n",
    "#Reading Walmart Closing Prices\n",
    "#Set csvpath to location of sp tsx csv file\n",
    "walmart_returns_csv = Path(\"Resources/walmart_historical.csv\")\n",
    "\n",
    "#Read in the CSV and create Dataframe, \n",
    "#Set index to date column and ensure dates are converted to a DateTimeIndex.\n",
    "#Parse dates using default pandas date parser\n",
    "walmart_returns = pd.read_csv(walmart_returns_csv, index_col='Date', infer_datetime_format=True, parse_dates = True)\n",
    "\n",
    "#Confirm dates are sorted in place in ascending manner\n",
    "walmart_returns.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "#Return the fist three rows of the walmart_returns data frame for inspection\n",
    "walmart_returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unwanted columns\n",
    "walmart_returns.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\", \"Unnamed: 5\", \"Unnamed: 6\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 10\", \"Unnamed: 11\", \"Unnamed: 12\", \"Unnamed: 13\", \"Unnamed: 14\", \"Unnamed: 15\", \"Unnamed: 16\", \"Unnamed: 17\", \"Unnamed: 18\", \"Unnamed: 19\", \"Unnamed: 20\", \"Unnamed: 21\", \"Unnamed: 22\", \"Unnamed: 23\", \"Unnamed: 24\", \"Unnamed: 25\"], axis=1, inplace=True)\n",
    "\n",
    "#Check dataframe and ensure it has been cleaned of unwanted columns\n",
    "walmart_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Nulls\n",
    "walmart_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls in place\n",
    "walmart_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "walmart_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a sample of the cleaned data frame and inspect to ensure only the columns and required data remains\n",
    "walmart_returns.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock 2 Amazon - Import and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from 2nd stock - Amazon\n",
    "#Reading Amazon Closing Prices\n",
    "#Set csvpath to location of sp tsx csv file\n",
    "amazon_returns_csv = Path(\"Resources/amazon_historical.csv\")\n",
    "\n",
    "#Read in the CSV and create Dataframe, \n",
    "#Set index to date column and ensure dates are converted to a DateTimeIndex.\n",
    "#Parse dates using default pandas date parser\n",
    "amazon_returns = pd.read_csv(amazon_returns_csv, index_col='Date', infer_datetime_format=True, parse_dates = True)\n",
    "\n",
    "#Confirm dates are sorted in place in ascending manner\n",
    "amazon_returns.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "#Return the fist three rows of the amazon_returns data frame for inspection\n",
    "amazon_returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove unwanted columns\n",
    "amazon_returns.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\", \"Unnamed: 5\", \"Unnamed: 6\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 10\", \"Unnamed: 11\", \"Unnamed: 12\", \"Unnamed: 13\", \"Unnamed: 14\", \"Unnamed: 15\", \"Unnamed: 16\", \"Unnamed: 17\", \"Unnamed: 18\", \"Unnamed: 19\", \"Unnamed: 20\", \"Unnamed: 21\", \"Unnamed: 22\", \"Unnamed: 23\", \"Unnamed: 24\", \"Unnamed: 25\"], axis=1, inplace=True)\n",
    "\n",
    "#Check dataframe and ensure it has been cleaned of unwanted columns\n",
    "amazon_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Nulls\n",
    "amazon_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls in place\n",
    "amazon_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "amazon_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a sample of the cleaned data frame and inspect to ensure only the columns and required data remains\n",
    "amazon_returns.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock 3 Apple - Import and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reading data from 3rd stock - Apple\n",
    "#Reading Apple Closing Prices\n",
    "#Set csvpath to location of sp tsx csv file\n",
    "apple_returns_csv = Path(\"Resources/apple_historical.csv\")\n",
    "\n",
    "#Read in the CSV and create Dataframe, \n",
    "#Set index to date column and ensure dates are converted to a DateTimeIndex.\n",
    "#Parse dates using default pandas date parser\n",
    "apple_returns = pd.read_csv(apple_returns_csv, index_col='Date', infer_datetime_format=True, parse_dates = True)\n",
    "\n",
    "#Confirm dates are sorted in place in ascending manner\n",
    "apple_returns.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "#Return the fist three rows of the apple_returns data frame for inspection\n",
    "apple_returns.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unwanted columns\n",
    "apple_returns.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\", \"Unnamed: 5\", \"Unnamed: 6\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 10\", \"Unnamed: 11\", \"Unnamed: 12\", \"Unnamed: 13\", \"Unnamed: 14\", \"Unnamed: 15\", \"Unnamed: 16\", \"Unnamed: 17\", \"Unnamed: 18\", \"Unnamed: 19\", \"Unnamed: 20\", \"Unnamed: 21\", \"Unnamed: 22\", \"Unnamed: 23\", \"Unnamed: 24\", \"Unnamed: 25\"], axis=1, inplace=True)\n",
    "\n",
    "#Check dataframe and ensure it has been cleaned of unwanted columns\n",
    "apple_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Nulls\n",
    "apple_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls in place\n",
    "apple_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "apple_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a sample of the cleaned data frame and inspect to ensure only the columns and required data remains\n",
    "apple_returns.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all stocks and calculate daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all stocks in a single DataFrame\n",
    "#Join walmart_returns, amazon_returns and apple_returns into a single DataFrame with columns for each portfolio's returns.\n",
    "my_portfolio_df = pd.concat([walmart_returns, amazon_returns, apple_returns], axis=\"columns\", join=\"inner\")\n",
    "\n",
    "#Return a sample of the new combined data frame for inspection\n",
    "my_portfolio_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset Date index\n",
    "#Not Required as Index columns and format was set during read_csv operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorganize portfolio data by having a column per symbol\n",
    "my_portfolio_df.columns = ['WMT', 'AMZN', 'AAPL']\n",
    "\n",
    "#Return a sample of the new combined data frame for inspection to confirm updated column names\n",
    "my_portfolio_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WMT     0\n",
       "AMZN    0\n",
       "AAPL    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate daily returns\n",
    "my_portfolio_df_returns = my_portfolio_df.pct_change()\n",
    "\n",
    "# Drop NAs from first row as this will always be NA due to no preceeding data for pct_change calculation\n",
    "my_portfolio_df_returns.dropna(inplace=True)\n",
    "\n",
    "#Check again for nulls and print sum to ensure data has been cleansed of any null values\n",
    "my_portfolio_df_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WMT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-05 16:00:00</th>\n",
       "      <td>-0.026143</td>\n",
       "      <td>0.142089</td>\n",
       "      <td>0.037991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06 16:00:00</th>\n",
       "      <td>-0.049237</td>\n",
       "      <td>0.019037</td>\n",
       "      <td>0.012097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07 16:00:00</th>\n",
       "      <td>-0.038137</td>\n",
       "      <td>0.015014</td>\n",
       "      <td>-0.030034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09 16:00:00</th>\n",
       "      <td>-0.112131</td>\n",
       "      <td>0.135234</td>\n",
       "      <td>-0.149131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-10 16:00:00</th>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.048697</td>\n",
       "      <td>0.017453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          WMT      AMZN      AAPL\n",
       "Date                                             \n",
       "2015-01-05 16:00:00 -0.026143  0.142089  0.037991\n",
       "2015-01-06 16:00:00 -0.049237  0.019037  0.012097\n",
       "2015-01-07 16:00:00 -0.038137  0.015014 -0.030034\n",
       "2015-01-09 16:00:00 -0.112131  0.135234 -0.149131\n",
       "2015-01-10 16:00:00  0.007051  0.048697  0.017453"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return a sample of the new combined data frame for inspection\n",
    "my_portfolio_df_returns.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2015-01-05 16:00:00    0.051312\n",
       "2015-01-06 16:00:00   -0.006034\n",
       "2015-01-07 16:00:00   -0.017719\n",
       "2015-01-09 16:00:00   -0.042009\n",
       "2015-01-10 16:00:00    0.024400\n",
       "dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set weights\n",
    "weights = [1/3, 1/3, 1/3]\n",
    "\n",
    "# Calculate portfolio return\n",
    "my_portfolio_df_returns_combined = my_portfolio_df_returns.dot(weights)\n",
    "\n",
    "# Display sample data\n",
    "my_portfolio_df_returns_combined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join your portfolio returns to the DataFrame that contains all of the portfolio returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join your returns DataFrame to the original returns DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only compare dates where return data exists for all the stocks (drop NaNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the risk analysis with your portfolio to see how it compares to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized `std`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling standard deviation\n",
    "\n",
    "# Plot rolling standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot the 60-day Rolling Beta for Your Portfolio compared to the S&P 60 TSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot Beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Annualized Sharpe Ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your portfolio do?\n",
    "\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
